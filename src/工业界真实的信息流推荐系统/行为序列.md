# 行为序列

<!-- toc -->

## 用户历史行为序列建模

用户最近 n 次点击、点赞、收藏、转发等行为都是推荐系统中重要的特征，可以帮助召回和排序变得更精准。这节课介绍最简单的方法——对用户行为取简单的平均，作为特征输入召回、排序模型。

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305093523.png)

last n物品 ->特征处理

### LastN特征

LastN：用户最近的n次交互（点击、点赞等）的物品ID。

对LastN 物品ID 做embedding，得到n个向量。

把n个向量取平均，作为用户的一种特征。

适用于召回双塔模型、粗排三塔模型、精排模型。

参考文献：
- Covington, Adams, and Sargin. Deep neural networks for YouTube recommendations. In ACM Conference on Recommender Systems, 2016

### 小红书的实践

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221104217.png)
## DIN模型（注意力机制）

上节课介绍了用户的 LastN 序列特征。这节课介绍 DIN 模型，它是对 LastN 序列建模的一种方法，效果优于简单的平均。DIN 的本质是注意力机制（attention）。DIN 是阿里在 2018 年提出的，有兴趣的话可以阅读下面的参考文献。  
  
参考文献：  
Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.

DIN用加权平均代替平均，即注意力机制(attention)。

权重：候选物品与用户LastN物品的相似度。


![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221104536.png)

### DIN模型

对于某候选物品，计算它与用户LastN物品的相似度。

以相似度为权重，求用户LastN物品向量的加权和，结果是一个向量。

把得到的向量作为一种用户特征，输入排序模型，预估（用户，候选物品）的点击率、点赞率等指标。

本质是注意力机制（attention）。

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305094010.png)
### 简单平均 V.S. 注意力机制

简单平均和注意力机制都适用于精排模型。

简单平均适用于双塔模型、三塔模型。简单平均只需要用到LastN，属于用户自身的特征。把LastN 向量的平均作为用户塔的输入。

注意力机制不适用于双塔模型、三塔模型。注意力机制需要用到LastN+候选物品。用户塔看不到候选物品，不能把注意力机制用在用户塔。

### DIN模型的缺点

注意力层的计算量正比于用户行为序列的长度$n$。

只能记录最近几百个物品，否则计算量太大。

缺点：关注短期兴趣，遗忘长期兴趣。

参考文献：
Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.

### 如何改进DIN？

目标：保留用户长期行为序列（n很大）而且计算量不会过大。

改进DIN：
- DIN对LastN向量做加权平均，权重是相似度。
- 如果某LastN物品与候选物品差异很大，则权重接近零。快速排除掉与候选物品无关的LastN
- 物品，降低注意力层的计算量。
## SIM模型（长序列模型）

这节课继续讲解推荐系统中的用户行为序列建模。这节课介绍 SIM 模型，它的主要目的是保留用户的长期兴趣。SIM 的原理是对用户行为序列做快速筛选，缩短序列长度，使得DIN可以用于长序列。  
  
参考文献：  
Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020.

### SIM模型特征

保留用户长期行为记录，n的大小可以是几千。

对于每个候选物品，在用户LastN记录中做快速查找，找到k个相似物品。

把LastN变成TopK，然后输入到注意力层。

SIM模型减小计算量（从n降到k）

参考文献：
Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020.

### 第一步：查找

方法一：Hard Search

- 根据候选物品的类目，保留LastN物品中类目相同的。
- 简单，快速，无需训练。

方法二：Soft Search

- 把物品做embedding，变成向量。
- 把候选物品向量作为query，做k近邻查找，保留LastN 物品中最接近的k个。
- 效果更好，编程实现更复杂。

### 第二步：注意力机制

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221105245.png)

trick：使用时间信息

- 用户与某个LastN物品的交互时刻距今为$\delta$。
- 对$\delta$做离散化，再做embedding，变成向量$d$。
- 把两个向量做concatenation，表征一个LastN物品。
	- 向量x是物品embedding
	- 向量d是时间的embedding

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305095419.png)

为什么SIM使用时间信息？

- DIN的序列短，记录用户近期行为。
- SIM的序列长，记录用户长期行为。
- 时间越久远，重要性越低。
### 结论

- 长序列（长期兴趣）优于短序列（近期兴趣）。
- 注意力机制优于简单平均。
- Soft search 还是hard search？取决于工程基建。
- 使用时间信息有提升。