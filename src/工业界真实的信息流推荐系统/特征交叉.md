# 特征交叉

<!-- toc -->

## Factorized Machine(FM) 因式分解机

线性模型对输入的特征取加权和，作为对目标的预估。如果先做特征交叉，再用线性模型，通常可以取得更好的效果。如果做二阶特征交叉，那么参数量为O(特征数量平方)，计算量大，而且容易造成过拟合。因式分解机（Factorized Machine, FM）用低秩矩阵分解的方式降低参数量，加速计算。任何可以用线性模型（比如线性回归、逻辑回归）解决的问题，都可以用 FM 解决。

### 线性模型

线性模型有d个特征，d+1个参数，预测是特征的加权和。（只有加，没有乘。）

### 二阶交叉特征

有d个特征，线性模型+二阶交叉特征模型有$O(d^2)$个参数。

下面考虑如何减少参数数量。

### 线性模型+二阶交叉特征 VS FM

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221102045.png)

### Factorized Machine

FM是线性模型的替代品，能用线性回归、逻辑回归的场景，都可以用FM。

FM使用二阶交叉特征，表达能力比线性模型更强。

通过做近似$u_{ij}≈ v_i^Tv_j$，FM 把二阶交叉权重的数量从$O(d^2)$降低到$O(kd)$ 。

现在，FM 已过时。

参考文献：
Steffen Rendle. Factorization machines. In ICDM, 2010

## DCN深度交叉网络

Deep & Cross Networks (DCN) 译作“深度交叉网络”，可以用于召回双塔模型、粗排三塔模型、精排模型。DCN 由一个深度网络和一个交叉网络组成，交叉网络的基本组成单元是交叉层 (Cross Layer)。这节课最重点的部分就是交叉层。

### 多目标预测

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305102214.png)

### MMoE

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305102241.png)

### 交叉层

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221102422.png)
![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221102553.png)

类似跳跃连接（可以防止梯度消失）

### 哈达玛乘积 (Hadamard product)

哈达玛乘积 (Hadamard product)，也称为 Schur 乘积或元素对应乘积 (element-wise product)，是矩阵的一种运算方式。它与标准的矩阵乘法不同。

**定义：**

给定两个维度相同的矩阵 A 和 B (例如，都是 m x n 的矩阵)，它们的哈达玛积 A ⊙ B 是一个同样维度的矩阵，其每个元素是由 A 和 B 相应位置的元素相乘得到的。

用数学公式表示：

$$(A ⊙ B)<sub>ij</sub> = A<sub>ij</sub> * B<sub>ij</sub>$$

其中：

*   A ⊙ B 表示矩阵 A 和 B 的哈达玛积
*   A<sub>ij</sub> 表示矩阵 A 的第 i 行第 j 列的元素
*   B<sub>ij</sub> 表示矩阵 B 的第 i 行第 j 列的元素
*   * 表示普通的标量乘法

**例子：**

假设有两个矩阵：

A =  
$$
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$

B =
$$
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
$$

那么它们的哈达玛积 A ⊙ B 为：

A ⊙ B =
$$
\begin{bmatrix}
1*5 & 2*6 \\
3*7 & 4*8
\end{bmatrix}
=
\begin{bmatrix}
5 & 12 \\
21 & 32
\end{bmatrix}
$$

**与标准矩阵乘法的区别：**

*   **哈达玛积：** 要求参与运算的矩阵维度完全相同，结果矩阵的维度也与输入矩阵相同，每个元素是对应位置元素相乘。
*   **标准矩阵乘法：** 要求第一个矩阵的列数等于第二个矩阵的行数，结果矩阵的维度取决于输入矩阵的维度，元素是通过行向量和列向量的点积计算得到的。

**应用：**

哈达玛积在很多领域都有应用，包括：

*   **图像处理：**  可以用于图像的像素级操作，例如改变图像的亮度或对比度。
*   **神经网络：** 在某些神经网络结构中，哈达玛积被用作一种非线性操作。
*   **量子信息：**  在量子计算中，哈达玛矩阵的哈达玛积被用来构建更复杂的量子门。
*   **推荐系统：** 可以用于计算用户或物品之间的相似度。
*   **信号处理：** 用于信号的调制和解调。

**总结：**

哈达玛积是一种简单但非常有用的矩阵运算，它将两个维度相同的矩阵对应位置的元素相乘。理解它的定义和与标准矩阵乘法的区别对于理解其应用至关重要。


### 交叉网络

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221102854.png)

### 参考资料

这节课介绍的是Cross Network V2[1]。

老版本的 Cross Network 在论文[2] 中提出。

参考文献：
1. Ruoxi Wang et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.
2. Ruoxi Wang et al. Deep & Cross Network for Ad Click Predictions. In ADKDD, 2017.

### DCN

DCN可以用于召回和排序

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221102936.png)


## 只用于精排的LHUC（PPNet）

这节课介绍 LHUC 这种神经网络结构，可以用于精排。LHUC 的起源是语音识别，后来被应用到推荐系统，快手将其称为 PPNet，现在已经在业界广泛落地。  
  
视频中遗漏一个细节：将LHUC用于推荐系统，门控神经网络（2 x sigmoid）的梯度不要传递到用户ID embedding特征，需要对其做 stop gradient。  
  
参考文献：  
1. Pawel Swietojanski, Jinyu Li, & Steve Renals. Learning hidden unit contributions for unsupervised acoustic model adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.  
2. 快手落地万亿参数推荐精排模型，2021。链接：https://ai.51cto.com/art/202102/644214.html

### Learning Hidden Unit Contributions (LHUC)

LHUC起源于语音识别[1]。

快手将LHUC应用在推荐精排[2]，称作PPNet。

参考文献：
1. Pawel Swietojanski, Jinyu Li, & Steve Renals. Learning hidden unit contributions for unsupervised acoustic model adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.
2. 快手落地万亿参数推荐精排模型，2021。链接： https:/ /ai.51cto.c0m/art/202102/644214.html

### 语音识别中的 LHUC

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221103213.png)

提取说话者个人特征
### 推荐系统排序模型中的 LHUC

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221103235.png)


## SENet和Bilinear交叉

这节课介绍几种方法：  
1. SENet 是计算机视觉中的一种技术，可以用在推荐系统中对特征做动态加权。  
2. 双线性（bilinear）特征交叉可以提升排序模型的表现。有很多种 bilinear 交叉的方法。  
3. FiBiNet 是将 SENet 与 Bilinear 模型结合。

### SENet

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305103356.png)

上图中把向量都映射成k维。

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305103449.png)

参考文献：

1. Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Networks. In CVPR, 2018.
2. Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction. In RecSys, 2019.

### SENet特点

SENet 对离散特征做 field-wise 加权。

Field：用户 ID Embedding 是 64 维向量。64个元素算一个field，获得相同的权重。

如果有m个fields，那么权重向量是m维。

### Field间特征交叉

特征交叉：内积和哈达玛乘积

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305103706.png)

Bilinear Cross（内积）

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250305103915.png)

在实践中，需要手动选择特征进行交叉，否则参数量会非常大！

### SENet和Field间特征交叉

- SENet 对离散特征做 field-wise 加权
- Field 间特征交叉:
	- 向量内积
	- 哈达玛乘积
	- Bilinear cross

### FiBiNet

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250221103842.png)

